{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import invgamma\n",
    "from geopy.distance import geodesic\n",
    "from dms_variants.ispline import Isplines, Isplines_total\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pymc as pm\n",
    "import sys\n",
    "\n",
    "# Add the module path to sys.path\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of zeros: 0.0\n",
      "Proportion of ones: 0.08947368421052632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_206594/501244899.py:91: RuntimeWarning: invalid value encountered in log\n",
      "  np.where(lm_mod.coef_ > 0, np.log(lm_mod.coef_), -10),  # Log coefficients greater than 0, and set those < 0 to -10\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------\n",
    "# load in and parse data\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "dat_all = pd.read_csv(\"../../data/sa_family_data.csv\").sample(n=20,random_state=1)\n",
    "\n",
    "# Parse data into location, environmental variables, and species cover/presence\n",
    "location_mat = dat_all[['latitude', 'longitude']].values\n",
    "env_var_names = ['gmap', 'RFL_CONC', 'Elevation30m', 'HeatLoadIndex30m', 'tmean13c', 'SoilConductivitymSm', 'SoilTotalNPercent']\n",
    "envr_use = dat_all[['gmap', 'RFL_CONC', 'Elevation30m', 'HeatLoadIndex30m', 'tmean13c', 'SoilConductivitymSm', 'SoilTotalNPercent']].values\n",
    "species_mat = dat_all.iloc[:, 12:].values\n",
    "\n",
    "# Get the number of sites\n",
    "ns = location_mat.shape[0]\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# Calculate Bray-Curtis dissimilarity -- see proportion of 0's and 1's\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "# Define dissimilarity metric\n",
    "diss_met = 'braycurtis'\n",
    "\n",
    "# Create pair-wise response matrix. pdist outputs a 1D matrix\n",
    "Z = pdist(species_mat, diss_met)\n",
    "\n",
    "# Find indices for observations with dissimilarity of exactly 1\n",
    "Z_is_one = np.where(Z == 1)[0]\n",
    "Z_is_not_one = np.where(Z != 1)[0]\n",
    "\n",
    "# Get counts\n",
    "n1 = len(Z_is_one)\n",
    "N = len(Z)\n",
    "\n",
    "# Print the proportion of zeros and ones in the dissimilarity matrix\n",
    "print(f\"Proportion of zeros: {np.mean(Z == 0)}\")\n",
    "print(f\"Proportion of ones: {np.mean(Z == 1)}\")\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# Define covariates that will be warped by I-spline function AND coefficients\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "# Calculate pairwise geographical distance in km using geodesic with WGS84 in vector form (upper triangle of matrix)\n",
    "vec_distance = pdist(location_mat, lambda lat, lon: geodesic(lat, lon).kilometers)\n",
    "\n",
    "# Define a (sub)set of environmental variables to be used as covariates\n",
    "X = envr_use[:, [0, 1, 2, 3, 4, 5, 6]]\n",
    "\n",
    "# How many knots do you want? What is the degree of the spline?\n",
    "# Remember that in the specification, of the iSpline that the degree is\n",
    "# one higher that what you say. Integration of m-spline adds one degree.\n",
    "# Define the degree and number of knots for the I-spline basis\n",
    "deg = 3\n",
    "knots = 2\n",
    "order = deg + 1\n",
    "df = deg * knots\n",
    "\n",
    "# Create ISpline bases, with different range (i.e. mesh) for each variable. Note NO extrapolation with this method, only interpolation.\n",
    "I_spline_bases = np.column_stack([Isplines(order, [X[:,i].min(), X[:,i].min() + (X[:,i].max() - X[:,i].min())/3, X[:,i].min() + (X[:,i].max() - X[:,i].min())*2/3, X[:,i].max()], X[:,i]).I(j) for i in range(X.shape[1]) for j in range(1,df)])\n",
    "\n",
    "# Pairwise differences of each basis function in a vector. Shape ns**2 / 2 x p * df\n",
    "I_spline_bases_diffs = np.array([pdist(I_spline_bases[:, i].reshape(-1, 1), metric='euclidean') for i in range(I_spline_bases.shape[1])]).T\n",
    "\n",
    "# Create spline for the geodesic distances\n",
    "dist_mesh = [vec_distance.min(), vec_distance.min() + (vec_distance.max() - vec_distance.min())/3, vec_distance.min() + (vec_distance.max() - vec_distance.min())*2/3, vec_distance.max()]\n",
    "dist_splines = np.column_stack([Isplines(order, dist_mesh, vec_distance).I(i) for i in range(1, df)])\n",
    "\n",
    "# Combine the I-spline bases and the geographical distances\n",
    "X_GDM = np.column_stack([I_spline_bases_diffs, dist_splines])\n",
    "\n",
    "# Name columns\n",
    "column_names = [f\"{var}_I{j}\" for var in env_var_names for j in range(1, df)] + [f\"Dist_I{j}\" for j in range(1, df)]\n",
    "X_GDM_df = pd.DataFrame(X_GDM, columns=column_names)\n",
    "p = X_GDM.shape[1]\n",
    "\n",
    "# Get row and column indices for each dissimilarity \n",
    "row_ind, col_ind = np.triu_indices(ns, k=1)\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# Get initial values to speed up spGDMM fitting\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# Fit linear model\n",
    "lm_mod = LinearRegression(fit_intercept=True).fit(X_GDM, np.log(Z))\n",
    "\n",
    "# Define objective function\n",
    "objective = lambda params: np.sum((np.log(Z) - params[0] - X_GDM @ np.exp(params[1:(p+1)]))**2)\n",
    "\n",
    "# Set up initial parameters, taking logs of positive and setting negative coefficients to -10\n",
    "initial_params = np.concatenate([\n",
    "    [0.3],  # Intercept, why chosen to be this?\n",
    "    np.where(lm_mod.coef_ > 0, np.log(lm_mod.coef_), -10),  # Log coefficients greater than 0, and set those < 0 to -10\n",
    "    #np.random.randn(ns)  # Random values for each site\n",
    "])\n",
    "\n",
    "optimized_params = minimize(objective, initial_params, method='BFGS').x\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# Fix spatial range parameter (rho = 1 / phi) heuristically (can also estimate using the data, or even deep GP). \n",
    "# Setting up spatial correlation structure.\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "rho_fix = np.max(vec_distance) / 10\n",
    "R_spat_mat = squareform(np.exp(- vec_distance / rho_fix))\n",
    "#chol_R = np.linalg.cholesky(R_spat_mat).T\n",
    "R_inv = np.linalg.inv(R_spat_mat)\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# Define design matrix for a polynomial log-variance\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# NOTE these features are not orthogonal as they were in the R implementation\n",
    "X_sigma = PolynomialFeatures(degree=3, include_bias=True).fit_transform(vec_distance.reshape(-1, 1))\n",
    "p_sigma = X_sigma.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harold/miniconda3/envs/spGDMM/lib/python3.12/site-packages/pymc/model/core.py:1366: ImputationWarning: Data in log_V_obs contains missing values and will be automatically imputed from the sampling distribution.\n",
      "  warnings.warn(impute_message, ImputationWarning)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [beta_0, beta, sigma2, log_V_obs_unobserved]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2174fe6720a4c20930dd6ec26fb44ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 157 seconds.\n",
      "/home/harold/miniconda3/envs/spGDMM/lib/python3.12/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n",
      "There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "Chain 0 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "Chain 1 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "Chain 2 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "Chain 3 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------------------------------------------------\n",
    "# Source nimble models -- Models 1-9 match those in paper\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# Create constants for the model\n",
    "constants = {\n",
    "    'n': N,\n",
    "    'p': p,\n",
    "    'x': X_GDM,\n",
    "}\n",
    "\n",
    "# # Create data for the model\n",
    "# data = {\n",
    "#     'log_V': np.where(Z == 1, np.nan, np.log(Z)),\n",
    "#     'censored': (Z == 1).astype(int),\n",
    "#     'c': np.zeros(constants['n'])\n",
    "# }\n",
    "\n",
    "# # Create initial values for the model\n",
    "# inits = {\n",
    "#     'beta_0': optimized_params[0],\n",
    "#     'log_beta': optimized_params[1:(p+1)],\n",
    "#     'sig2_psi': 1,\n",
    "#     'beta_sigma': np.array([-5, -20, 12, 2]),\n",
    "#     'psi': optimized_params[(p+1):]\n",
    "# }\n",
    "'''\n",
    "Implement the first model using PyMC. Provided parameters are for a specific site-pair.\n",
    "    \n",
    "    V ~ N(mu, sigma^2)\n",
    "    mu = beta_0 + beta * h(||s[i] - s[j]||) + sum of the covariate distances\n",
    "\n",
    "    Spatial Random Effects -    None\n",
    "    Variance -                  sigma^2\n",
    "\n",
    "    Parameters:\n",
    "    X (ndarray): The feature matrix.\n",
    "    p (int): The number of features.\n",
    "    n (int): The number of observations.\n",
    "    c (ndarray): The censoring values.\n",
    "    log_V (ndarray): The log-transformed response variable.\n",
    "    censored (ndarray): The censored observations.\n",
    "'''\n",
    "# CHECK IF THERE IS AN INTERCEPT AT THE BEGINNING OF OPTIMIZED PARAMS & X_GDM\n",
    "# CHECK IF WE ARE USING PYMC ON THE ENTIRE X_GDM\n",
    "beta_0_init = optimized_params[0]\n",
    "log_beta_init =  optimized_params[1:(p+1)]\n",
    "sig2_psi_init = 1\n",
    "beta_sigma_init = np.array([-5, -20, 12, 2])\n",
    "psi_init = optimized_params[(p+1):]\n",
    "\n",
    "log_V = np.where(Z == 1, np.nan, np.log(Z))\n",
    "censored = 1*(Z == 1)\n",
    "c = np.zeros(N)\n",
    "\n",
    "print(log_V.shape)\n",
    "\n",
    "with pm.Model() as model:\n",
    "        # Define the priors\n",
    "        beta_0 = pm.Normal('beta_0', mu=0, sigma=10)\n",
    "        beta = pm.Lognormal('beta', mu=0, sigma=10, shape=p)\n",
    "\n",
    "        # Calculate the linear predictor\n",
    "        linpred = pm.math.dot(X_GDM, beta)\n",
    "        \n",
    "        # Define the likelihood\n",
    "        sigma2 = pm.InverseGamma('sigma2', alpha=1, beta=1)\n",
    "        mu = beta_0 + linpred\n",
    "\n",
    "        censored = pm.Bound(pm.Normal, lower=0)('censored', mu=mu, sigma=np.sqrt(sigma2), observed=c)\n",
    "\n",
    "        log_V = pm.Normal('log_V', mu=mu, sigma=np.sqrt(sigma2), shape=n)\n",
    "\n",
    "        # log_V_obs = pm.Normal('log_V_obs', mu=mu, sigma=pm.math.sqrt(sigma2), observed=log_V)\n",
    "        \n",
    "        # Sample from the posterior with progress bar enabled\n",
    "        trace = pm.sample(1000, return_inferencedata=True, progressbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apply', 'BatchedDiag', 'LogDet', 'Op', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'abs', 'and_', 'arccos', 'arccosh', 'arcsin', 'arcsinh', 'arctan', 'arctanh', 'batched_diag', 'block_diagonal', 'broadcast_to', 'cartesian', 'ceil', 'clip', 'concatenate', 'constant', 'cos', 'cosh', 'cumprod', 'cumsum', 'dot', 'eq', 'erf', 'erfc', 'erfcinv', 'erfinv', 'exp', 'expand_packed_triangular', 'flat_outer', 'flatten', 'flatten_list', 'floatX', 'floor', 'full', 'full_like', 'ge', 'gt', 'invlogit', 'invprobit', 'kron_diag', 'kron_dot', 'kron_matrix_op', 'kron_solve_lower', 'kron_solve_upper', 'kronecker', 'le', 'log', 'log1mexp', 'log1mexp_numpy', 'log1pexp', 'log_softmax', 'logaddexp', 'logbern', 'logdet', 'logdiffexp', 'logdiffexp_numpy', 'logit', 'logsumexp', 'lt', 'matmul', 'matrix_inverse', 'max', 'maximum', 'mean', 'min', 'minimum', 'neq', 'np', 'ones', 'ones_like', 'or_', 'partial', 'probit', 'prod', 'pt', 'pytensor', 'reduce', 'round', 'sgn', 'sigmoid', 'sin', 'sinh', 'softmax', 'solve_triangular', 'sqr', 'sqrt', 'stack', 'sum', 'switch', 'sys', 'tan', 'tanh', 'warnings', 'where', 'zeros', 'zeros_like']\n",
      "dot.0\n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "\n",
    "# Check if pm.math is available\n",
    "print(dir(pm.math))\n",
    "\n",
    "# Example usage of pm.math.dot\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.randn(100, 5)\n",
    "beta = np.random.randn(5)\n",
    "\n",
    "linpred = pm.math.dot(X, beta)\n",
    "print(linpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.16.2\n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "print(pm.__version__)\n",
    "from pymc import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spGDMM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
